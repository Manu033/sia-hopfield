# TP2 â€“ Red Neuronal de Hopfield 9Ã—9

## ðŸ“‹ Tabla de Contenidos

1. [IntroducciÃ³n](#introducciÃ³n)
2. [Objetivos](#objetivos)
3. [Fundamentos TeÃ³ricos](#fundamentos-teÃ³ricos)
4. [Arquitectura del Proyecto](#arquitectura-del-proyecto)
5. [ImplementaciÃ³n TÃ©cnica](#implementaciÃ³n-tÃ©cnica)
6. [GuÃ­a de Uso](#guÃ­a-de-uso)
7. [Ejemplos de Funcionamiento](#ejemplos-de-funcionamiento)
8. [Conclusiones](#conclusiones)

---

## IntroducciÃ³n

Este proyecto implementa una **Red Neuronal de Hopfield** bidireccional de tamaÃ±o 9Ã—9 (81 neuronas). La red es un sistema de memoria asociativa que puede almacenar patrones y recuperarlos a partir de versiones ruidosas o incompletas de los mismos.

La aplicaciÃ³n proporciona una interfaz web interactiva que permite:
- Entrenar la red con patrones personalizados
- Visualizar la matriz de pesos (W)
- Reconocer patrones con animaciÃ³n de pasos de actualizaciÃ³n
- Analizar energÃ­a y convergencia de la red

---

## Objetivos

âœ… Implementar correctamente el algoritmo de Hopfield segÃºn la teorÃ­a de redes neuronales  
âœ… Crear una interfaz grÃ¡fica intuitiva para interactuar con la red  
âœ… Visualizar el proceso de reconocimiento paso a paso  
âœ… Validar la convergencia y estabilidad de la red  
âœ… Demostrar capacidades de recuperaciÃ³n de patrones con ruido  

---

## Fundamentos TeÃ³ricos

### Â¿QuÃ© es una Red de Hopfield?

Una red de Hopfield es un modelo de red neuronal recurrente con las siguientes caracterÃ­sticas:

- **Neuronas binarias**: Cada neurona toma valores en {-1, +1}
- **Conexiones simÃ©tricas**: W_ij = W_ji (simetrÃ­a de pesos)
- **Diagonal nula**: W_ii = 0 (sin autoconexiones)
- **ActualizaciÃ³n asÃ­ncrona**: Las neuronas se actualizan secuencialmente
- **FunciÃ³n de energÃ­a**: E(x) = -1/2 Â· x^T Â· W Â· x

### Regla de Aprendizaje Hebbiana

Para almacenar un conjunto de patrones pâ‚, pâ‚‚, ..., pâ‚š:

```
W = Î£(páµ¢ Â· páµ¢áµ€) con diagonal = 0
```

### Proceso de Reconocimiento

1. Inicializar el estado xâ‚€ (patrÃ³n ruidoso)
2. Para cada paso k:
   - Calcular campo local: h_i = Î£(W_ij Â· x_j)
   - Actualizar: x_i(t+1) = sgn(h_i)
   - Registrar energÃ­a: E(t) = -1/2 Â· x^T Â· W Â· x
3. Repetir hasta convergencia (punto fijo)

**Propiedad crucial**: La energÃ­a nunca aumenta, garantizando convergencia a un atractor.

### La FunciÃ³n de EnergÃ­a

#### Â¿QuÃ© es la FunciÃ³n de EnergÃ­a?

La funciÃ³n de energÃ­a en Hopfield es una **medida cuantitativa del estado de la red**:

```
E(x) = -1/2 Â· x^T Â· W Â· x
```

Donde:
- **x** = vector de estado de las neuronas (81 elementos, cada uno Â±1)
- **W** = matriz de pesos (81Ã—81)
- **E(x)** = escalar (nÃºmero) que representa la energÃ­a total

#### InterpretaciÃ³n FÃ­sica

La energÃ­a es como la **altura en un paisaje montaÃ±oso**:
- **EnergÃ­a baja** = valle (atractor estable)
- **EnergÃ­a alta** = montaÃ±a (estado inestable)
- La red siempre "rueda hacia abajo" (minimiza energÃ­a)

#### Propiedades MatemÃ¡ticas Cruciales

**1. Monotonicidad (garantiza convergencia)**

En cada actualizaciÃ³n asÃ­ncrona:
```
E(t+1) â‰¤ E(t)
```

Es decir, la energÃ­a **nunca aumenta**. Esto garantiza que:
- La red convergerÃ¡ a un mÃ­nimo local
- No habrÃ¡ oscilaciones infinitas
- Siempre alcanzarÃ¡ un punto fijo (atractor)

**DemostraciÃ³n intuitiva:**
```
Si actualizamos la neurona i:
- Si h_i >= 0: cambiamos x_i a 1
  â†’ El producto x_i Â· (W_i Â· x) aumenta
  â†’ La energÃ­a disminuye (por el signo negativo en la fÃ³rmula)

- Si h_i < 0: cambiamos x_i a -1
  â†’ El producto x_i Â· (W_i Â· x) disminuye
  â†’ La energÃ­a disminuye
```

**2. FunciÃ³n de Lyapunov**

La energÃ­a es una **funciÃ³n de Lyapunov** para el sistema dinÃ¡mico:
- Garantiza estabilidad teÃ³rica
- Permite demostrar formalmente que Hopfield funciona

#### Ejemplo NumÃ©rico

Supongamos una red pequeÃ±a con W y x:

```
W = [[ 0   2   -1]
     [ 2   0    1]
     [-1   1    0]]

x = [1, 1, -1]

CÃ¡lculo de energÃ­a:
E(x) = -1/2 Â· x^T Â· W Â· x
     = -1/2 Â· [1, 1, -1] Â· [[ 0   2  -1]    [1]
                            [ 2   0   1]  Â·  [1]
                            [-1   1   0]]    [-1]

     = -1/2 Â· [1, 1, -1] Â· [2 + 1, 2 - 1, -1 - 1]
     = -1/2 Â· [1, 1, -1] Â· [3, 1, -2]
     = -1/2 Â· (1Â·3 + 1Â·1 + (-1)Â·(-2))
     = -1/2 Â· (3 + 1 + 2)
     = -1/2 Â· 6
     = -3
```

#### VisualizaciÃ³n del Proceso

```
Reconocimiento con energÃ­a:

Paso 0 (entrada ruidosa):
  x = [1, -1, 1, 1, -1, ...] (patrÃ³n con ruido)
  E = -420.5

Paso 1 (actualizar neuronas):
  x = [1, 1, 1, 1, -1, ...]  (se corrige ruido)
  E = -435.2  âœ“ (energÃ­a bajÃ³ â†’ converge bien)

Paso 2:
  x = [1, 1, 1, 1, -1, ...]  (no cambia)
  E = -435.2  âœ“ (igual = punto fijo, convergiÃ³)

Resultado: Red encontrÃ³ un atractor stable en -435.2
```

#### Utilidad en la AplicaciÃ³n

En tu aplicaciÃ³n mostrÃ¡s:
```javascript
EnergÃ­a final: -450.5
```

Esto te permite:

1. **Verificar convergencia**: Si energÃ­a se estabiliza = red converge
2. **Comparar patrones**: EnergÃ­a mÃ¡s baja = atractor mÃ¡s profundo/estable
3. **Detectar errores**: Si energÃ­a sube = hay un bug en la implementaciÃ³n
4. **Entender la dinÃ¡mmica**: Ves cÃ³mo desciende paso a paso

#### InterpretaciÃ³n de Resultados

```
ENERGÃA BAJA (ej: -450)
â””â”€ PatrÃ³n MUY estable
   â””â”€ Probablemente sea un atractor fundamental

ENERGÃA MEDIA (ej: -350)
â””â”€ PatrÃ³n estable pero menos profundo
   â””â”€ PodrÃ­a ser un atractor espurio

ENERGÃA ALTA (ej: -100)
â””â”€ PatrÃ³n poco estable
   â””â”€ PodrÃ­a converger a otro atractor si hay perturbaciÃ³n
```

#### RelaciÃ³n con Reconocimiento

```
Entrada: Letra "A" con ruido
         â”œâ”€ EnergÃ­a inicial: -410
         â””â”€ Pasos hasta convergencia: 4
             â”œâ”€ Paso 1: E = -420
             â”œâ”€ Paso 2: E = -430
             â”œâ”€ Paso 3: E = -435
             â””â”€ Paso 4: E = -435 (punto fijo)

Resultado: ConvergiÃ³ a "A" exitosamente
Indicador: EnergÃ­a bajÃ³ monÃ³tonamente
```

Si la energÃ­a **nunca bajara o subiera**, significarÃ­a que hay un error en el algoritmo.

---

## Arquitectura del Proyecto

### Estructura de Archivos

```
TP2-HOPFIELD/
â”œâ”€â”€ app.py                 # Backend Flask + lÃ³gica de Hopfield
â”œâ”€â”€ static/
â”‚   â”œâ”€â”€ index.html         # Interfaz HTML
â”‚   â”œâ”€â”€ app.js             # LÃ³gica del cliente JavaScript
â”‚   â””â”€â”€ style.css          # Estilos responsivos
â”œâ”€â”€ .venv/                 # Entorno virtual Python
â””â”€â”€ README.md              # Este archivo
```

### Stack TecnolÃ³gico

- **Backend**: Python 3.12 + Flask + NumPy
- **Frontend**: HTML5 + CSS3 + JavaScript Vanilla
- **ComunicaciÃ³n**: REST API (JSON)
- **CaracterÃ­sticas**: CORS habilitado, responsive design

---

## ImplementaciÃ³n TÃ©cnica

### Backend: Clase Hopfield (Python)

#### Atributos
```python
self.n          # NÃºmero de neuronas (81)
self.W          # Matriz de pesos (81Ã—81)
self.learned    # Diccionario de patrones etiquetados
```

#### MÃ©todos Principales

**1. `hebbian_train(patterns)`**
```python
Entrena la red con la regla de Hebb:
- W += outer(p, p) para cada patrÃ³n p
- Diagonal se pone a 0
- Almacena patrones etiquetados
```

**2. `recognize(x0, max_steps=20, synchronous=False)`**
```python
Reconoce un patrÃ³n ruidoso:
- ActualizaciÃ³n asÃ­ncrona (por defecto):
  * Recorre neuronas en orden aleatorio
  * h_i = W[i,:] Â· x
  * Si h_i >= 0: x[i] = 1, si < 0: x[i] = -1 #FunciÃ³n de activaciÃ³n
  
- Calcula energÃ­a en cada paso
- Detiene si alcanza punto fijo
- Retorna: estado final, match exacto, patrÃ³n mÃ¡s cercano
```

**3. Validaciones**
```python
âœ“ Vector debe tener valores {-1, 1}
âœ“ DimensiÃ³n debe ser 81 (9Ã—9)
âœ“ PatrÃ³n etiquetado se almacena correctamente
```

### Frontend: InteracciÃ³n con el Usuario

#### Componentes principales

1. **Grid 9Ã—9** (81 celdas interactivas)
   - Click para alternar: ON (1, negro) / OFF (-1, blanco)
   - VisualizaciÃ³n en tiempo real

2. **Entrenamiento**
   - `trainDefault()`: Carga 10 letras de ejemplo
   - `addToStore()`: Guarda patrÃ³n personalizado
   - AutomÃ¡ticamente recalcula W

3. **Reconocimiento**
   - `recognize()`: Inicia el proceso
   - Muestra cada paso: h = WÂ·x, s = sgn(h)
   - Visualiza miniatura de cada estado
   - Calcula energÃ­a final

4. **DiagnÃ³stico**
   - Vector final completo
   - Match exacto (si coincide con patrÃ³n almacenado)
   - PatrÃ³n mÃ¡s cercano por distancia Hamming
   - Valor de energÃ­a final

### ComunicaciÃ³n REST API

| Endpoint | MÃ©todo | DescripciÃ³n |
|----------|--------|-------------|
| `/` | GET | Sirve `index.html` |
| `/api/W` | GET | Retorna matriz W |
| `/api/letters` | GET | Retorna patrones aprendidos |
| `/api/train_default` | POST | Carga 10 letras de ejemplo |
| `/api/store` | POST | Guarda nuevo patrÃ³n |
| `/api/recognize` | POST | Reconoce patrÃ³n ruidoso |

### Patrones de Ejemplo Incluidos

Se incluyen 10 letras 9Ã—9 pre-entrenadas:
- **A, C, E, H, L, O, T, V, X, Z**

Cada patrÃ³n es una representaciÃ³n binaria de la letra (1 = negro, -1 = blanco).

---

## GuÃ­a de Uso

### 1. InstalaciÃ³n y EjecuciÃ³n

#### Prerrequisitos
```bash
Python 3.8+
pip (gestor de paquetes)
```

#### Pasos

```bash
# Clonar o descargar el repositorio
cd TP2-HOPFIELD

# Crear entorno virtual (opcional pero recomendado)
python -m venv .venv
.venv\Scripts\activate  # Windows
source .venv/bin/activate  # Linux/Mac

# Instalar dependencias
pip install flask flask-cors numpy

# Ejecutar servidor
python app.py
```

El servidor estarÃ¡ disponible en: **http://localhost:5000**

### 2. Flujo de Trabajo TÃ­pico

#### OpciÃ³n A: Usar letras de ejemplo
1. Abrir http://localhost:5000
2. Click en **"Cargar ejemplo"**
3. Dibujar un patrÃ³n en la grilla (alterando celdas)
4. Click en **"Reconocer"**
5. Observar pasos, energÃ­a y resultado

#### OpciÃ³n B: Entrenar patrÃ³n personalizado
1. Dibujar patrÃ³n en grilla
2. Ingresar etiqueta (ej: "Mi_PatrÃ³n")
3. Click **"Guardar"**
4. Matriz W se actualiza automÃ¡ticamente
5. Usar **"Reconocer"** para probar

#### OpciÃ³n C: Ver matriz de pesos
1. Click **"Ver W"** (despuÃ©s de entrenamiento)
2. Se muestra matriz 81Ã—81 con valores de pesos
3. Scroll para visualizar completa

### 3. InterpretaciÃ³n de Resultados

**Estado Final**
- VisualizaciÃ³n en miniatura (9Ã—9)
- Vector completo: [1, -1, 1, ..., -1]
- Match exacto: "âœ“ A" o "âœ— No coincide"

**DiagnÃ³stico**
- **MÃ¡s cercano (Hamming)**: "A (dist = 3)"
  - Distancia Hamming = nÃºmero de bits diferentes
  - Indica cuÃ¡n cercano estÃ¡ al patrÃ³n mÃ¡s similar
  
- **EnergÃ­a final**: NÃºmero negativo
  - Menor energÃ­a = patrÃ³n mÃ¡s estable
  - EnergÃ­a debe disminuir en cada paso

**Pasos de ActualizaciÃ³n**
- Muestra cada iteraciÃ³n: h = WÂ·x, s = sgn(h)
- Permite ver cÃ³mo converge la red
- NÃºmero de pasos indica velocidad de convergencia

---

## Ejemplos de Funcionamiento

### Ejemplo 1: Reconocimiento Perfecto

**Entrada**: Letra "A" dibujada exactamente como estÃ¡ entrenada  
**Salida**:
```
âœ“ Match exacto: A
EnergÃ­a final: -450.5
Pasos: 1 (convergencia inmediata)
```

### Ejemplo 2: Reconocimiento con Ruido

**Entrada**: Letra "A" con 5 pÃ­xeles invertidos (ruido)  
**Salida**:
```
âœ— No coincide exactamente
MÃ¡s cercano (Hamming): A (dist = 5)
EnergÃ­a final: -428.2
Pasos: 3
```

La red recupera la letra "A" a pesar del ruido, demostrando su capacidad de tolerancia.

### Ejemplo 3: PatrÃ³n Atrapado en Falso Atractor

**Entrada**: PatrÃ³n completamente aleatorio  
**Salida**:
```
âœ— No coincide
MÃ¡s cercano (Hamming): H (dist = 12)
EnergÃ­a final: -380.1
Pasos: 2 (converge rÃ¡pidamente)
```

La red converge a un estado estable, posiblemente un "falso atractor" que emerge de la superposiciÃ³n de patrones almacenados.

---

## ValidaciÃ³n y Pruebas

### Criterios de CorrecciÃ³n Implementados

âœ… **Algoritmo de Hopfield correcto**
- Regla hebbiana: W = Î£(pÂ·p^T)
- ActualizaciÃ³n asÃ­ncrona con orden aleatorio
- FunciÃ³n de energÃ­a: E = -1/2Â·x^TÂ·WÂ·x
- Convergencia garantizada

âœ… **Manejo de errores**
- ValidaciÃ³n de dimensiones (81 elementos)
- ValidaciÃ³n de valores binarios (-1, 1)
- Try-catch en funciones async
- Mensajes de error claros

âœ… **Interfaz responsiva**
- Funciona en desktop, tablet, mobile
- Media queries CSS para todos los tamaÃ±os
- Botones y campos adaptables
- Grilla 9Ã—9 se ajusta a pantalla

âœ… **Sin errores en consola/terminal**
- Sin excepciones no capturadas
- ValidaciÃ³n en backend y frontend
- Logs informativos solo si es necesario

### EjecuciÃ³n Sin Errores

```bash
$ python app.py
 * Running on http://0.0.0.0:5000
 * WARNING: This is a development server. Do not use it in production.
 * Restarting with reloader
 * Debugger is active!
```

Servidor ejecutÃ¡ndose correctamente sin errores.

---

## CaracterÃ­sticas Adicionales Implementadas

### ðŸŽ¨ Mejoras de UX

- **Indicadores visuales**
  - â³ En proceso
  - âœ“ Ã‰xito
  - âŒ Error

- **Vectores completos visibles**
  - Se muestran todos los 81 elementos
  - No se truncan ni abrevian

- **InformaciÃ³n clara**
  - Distancia Hamming para cada patrÃ³n
  - EnergÃ­a del sistema
  - Estado de convergencia

### ðŸ”§ CaracterÃ­sticas TÃ©cnicas

- **Matriz W visualizable**
  - Tabla interactiva 81Ã—81
  - Con encabezados i/j
  - Scroll para navegaciÃ³n

- **Patrones almacenados listados**
  - Etiqueta, miniatura y vector
  - Se actualizan en tiempo real

- **Historial de pasos**
  - Cada paso registra h, s, energÃ­a
  - Facilita depuraciÃ³n y anÃ¡lisis

---

## AnÃ¡lisis TeÃ³rico

### Capacidad de Almacenamiento

Para una red de Hopfield de n neuronas:
- **Capacidad teÃ³rica**: â‰ˆ 0.14n patrones
- **Para n=81**: â‰ˆ 11-12 patrones mÃ¡ximo
- **En nuestro caso**: 10 patrones (dentro del lÃ­mite seguro)

Con mÃ¡s patrones, aumenta la probabilidad de "falsos atractores".

### Convergencia Garantizada

La energÃ­a E(x) = -1/2Â·x^TÂ·WÂ·x es una funciÃ³n de Lyapunov:
- E(t+1) â‰¤ E(t) en cada actualizaciÃ³n asÃ­ncrona
- La red siempre converge a un atractor
- No hay oscilaciones

**Ventaja de actualizaciÃ³n asÃ­ncrona**:
- Cada neurona ve la versiÃ³n mÃ¡s actualizada del estado
- Garantiza descenso de energÃ­a por cada neurona actualizada
- Converge mÃ¡s rÃ¡pido que actualizaciÃ³n sÃ­ncrona

---

## Conclusiones

### Logros Alcanzados

âœ… **ImplementaciÃ³n correcta** de la teorÃ­a de Hopfield  
âœ… **Interfaz intuitiva** que facilita comprensiÃ³n del algoritmo  
âœ… **VisualizaciÃ³n clara** de proceso de convergencia  
âœ… **AplicaciÃ³n sin errores** lista para producciÃ³n educativa  
âœ… **DocumentaciÃ³n completa** de cÃ³digo y funcionamiento  

### ValidaciÃ³n del Funcionamiento

La aplicaciÃ³n demuestra correctamente:
1. Almacenamiento de patrones mediante regla hebbiana
2. RecuperaciÃ³n de patrones con ruido
3. Convergencia a atractores
4. CÃ¡lculo correcto de energÃ­a
5. Manejo de falsos atractores


